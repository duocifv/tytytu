# chains/brand_chain.py
from typing import Dict, Any
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from services.llm_service import llm  # Gemini instance

def run_brand_chain(input_text: str) -> Dict[str, Any]:
    """
    Node 4: Quáº£n trá»‹ ThÆ°Æ¡ng hiá»‡u
    - Kiá»ƒm tra tone, hÃ¬nh áº£nh
    - Äá»‘i chiáº¿u guideline
    - Duyá»‡t Ã½ tÆ°á»Ÿng ban Ä‘áº§u
    """

    # 1ï¸âƒ£ Táº¡o prompt templates
    p1 = ChatPromptTemplate.from_template(
        "Dá»±a trÃªn ná»™i dung: {input_text}, kiá»ƒm tra tone vÃ  hÃ¬nh áº£nh cÃ³ phÃ¹ há»£p khÃ´ng."
    )
    p2 = ChatPromptTemplate.from_template(
        "Dá»±a trÃªn káº¿t quáº£ kiá»ƒm tra tone/hÃ¬nh áº£nh: {step1_tone}, Ä‘á»‘i chiáº¿u vá»›i guideline cá»§a thÆ°Æ¡ng hiá»‡u."
    )
    p3 = ChatPromptTemplate.from_template(
        "Dá»±a trÃªn Ä‘á»‘i chiáº¿u guideline: {step2_guideline}, duyá»‡t Ã½ tÆ°á»Ÿng ban Ä‘áº§u vÃ  tá»•ng há»£p káº¿t quáº£."
    )

    # 2ï¸âƒ£ Táº¡o chain chuáº©n LangChain 2025 vá»›i RunnableLambda Ä‘á»ƒ feed dict
    chain3 = (
        p1 | llm
        | RunnableLambda(lambda step1_tone: {"step1_tone": step1_tone}) | p2 | llm
        | RunnableLambda(lambda step2_guideline: {"step2_guideline": step2_guideline}) | p3 | llm
    )

    # 3ï¸âƒ£ Invoke chain vá»›i input_text
    final_result = chain3.invoke({"input_text": input_text})

    # 4ï¸âƒ£ Log thÃ´ng tin
    print("   â”œâ”€ ğŸ¨ Kiá»ƒm tra tone, hÃ¬nh áº£nh")
    print("   â”œâ”€ ğŸ“š Äá»‘i chiáº¿u guideline")
    print("   â””â”€ âœ… Duyá»‡t Ã½ tÆ°á»Ÿng ban Ä‘áº§u")

    return final_result
