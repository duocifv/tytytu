import uuid
import time
import threading
from fastapi import FastAPI
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import HumanMessage
from brain.nodes import build_graph
import uvicorn
from contextlib import asynccontextmanager
from brain.types import State
from flows.workflow_run import start_workflow

# # Bi·∫øn ƒëi·ªÅu khi·ªÉn
# running = False
# graph = None


# def setup():
#     """Kh·ªüi t·∫°o workflow"""
#     memory = MemorySaver()
#     workflow = build_graph()
#     return workflow.compile(checkpointer=memory)


# def loop():
#     """Loop workflow khi b·∫≠t"""
#     global running, graph
#     while running:
#         thread_id = str(uuid.uuid4())
#         config = {"configurable": {"thread_id": thread_id}}

#         init_state: State = {
#             "messages": [HumanMessage(content="B·∫Øt ƒë·∫ßu workflow")],
#             "outputs": {
#                 "keyword": [],
#                 "research": [],
#                 "insight": [],
#                 "idea": [],
#                 "title": "",
#                 "content": "",
#                 "publish": {}
#             },
#             "retries": {},
#             "topic": "S·ª©c kh·ªèe",
#             "status": {
#                 "sequence": 0,
#                 "step": "init",
#                 "done_nodes": []
#             }
#         }

#         # 1. Ch·∫°y workflow
#         state1 = graph.invoke(init_state, config=config)
#         print("K·∫øt qu·∫£ workflow:")
#         for msg in state1["messages"]:
#             print("-", msg.content)

#         # 2. Resume tr∆∞·ªõc publish
#         history = list(graph.get_state_history(config))
#         cp = next(
#             (
#                 h for h in reversed(history)
#                 if not any(
#                     "publish" in (m.content if hasattr(m, "content") else str(m)).lower()
#                     for m in h.values.get("messages", [])
#                 )
#             ),
#             None
#         )

#         if cp:
#             resumed_state = graph.invoke(
#                 None,
#                 config={
#                     "configurable": {
#                         "thread_id": thread_id,
#                         "checkpoint_id": cp.config["configurable"]["checkpoint_id"]
#                     }
#                 },
#             )
#             print("\nK·∫øt qu·∫£ resume tr∆∞·ªõc publish:")
#             for msg in resumed_state["messages"]:
#                 print("-", msg.content)

#         time.sleep(5)  # gi·ªëng Arduino delay


# @asynccontextmanager
# async def lifespan(app: FastAPI):
#     global graph
#     # ch·∫°y khi app kh·ªüi ƒë·ªông
#     graph = setup()
#     yield
#     # cleanup khi app t·∫Øt (n·∫øu c·∫ßn)


app = FastAPI()


@app.post("/start")
def start_loop():
    print("üì© /start API called")
    start_workflow()
    return {"status": "already running"}


@app.post("/stop")
def stop_loop():
    global running
    running = False
    return {"status": "stopped"}


@app.get("/status")
def get_status():
    return {"running": running}


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
